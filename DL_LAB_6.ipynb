{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAHmMy7kU1/4MtPmg1rksQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manoj-dotcom/Deep-Learning/blob/main/DL_LAB_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ggk26YfvozeA",
        "outputId": "053bf581-afc9-4254-d288-012571338ce0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifying newswires: a multi-class classification example"
      ],
      "metadata": {
        "id": "1dB2ScpjpUw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this problem we use Reuters dataset to classify the newswires. Reuters newswires into 46 mutually exclusive topics. Hence we build a neural network that classifies the 46 classes. Hence it is a multiclassification problem. We need to classify that each topic belongs to a single label or not. ( whether it is poisitiv or not). Hence we have only one label, it is a single labeled-multi classification problem."
      ],
      "metadata": {
        "id": "_1OOnXvjpXsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Reuters dataset"
      ],
      "metadata": {
        "id": "6cshLyW8pk05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Reuters dataset, a set of short newswires and their topics, published by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set. Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras."
      ],
      "metadata": {
        "id": "-hdHQ9j7puyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Loading the Reuters dataset**"
      ],
      "metadata": {
        "id": "bYc-_uHrpxDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "dsfUcAaApMJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the argument num_words=10000 restricts the data to the 10,000 most frequently occurring words found in the data.\n",
        "\n",
        "We have 8,982 training examples and 2,246 test examples:"
      ],
      "metadata": {
        "id": "-Z6rnVH4rDXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M61xnSrrAvj",
        "outputId": "a8a05675-9927-486f-a591-2a42cecfed53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwQKf9VvrF_T",
        "outputId": "351aacd5-1bd8-42b8-b05b-4c383b1fccff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDUAVIcYrHiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how you can decode it back to words, in case you are curious:"
      ],
      "metadata": {
        "id": "GxKE8XSgrKii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "metadata": {
        "id": "HI2Dr5X3rJCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”"
      ],
      "metadata": {
        "id": "NtKoLVv7r1c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_newswire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nzab7DiWrRp6",
        "outputId": "776e6a7f-57bc-4b39-b5bd-fea9dbe20716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the data\n",
        "\n",
        "You can vectorize the data with the exact same code. ( in the format word to vector)"
      ],
      "metadata": {
        "id": "9O4TaqIBrWJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "x_train,x_train.shape,x_train.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJisUFQ-rT6T",
        "outputId": "6722d6cb-03e8-4a66-df81-d5f12e4614ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.]]),\n",
              " (8982, 10000),\n",
              " 2)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "5Z8HzZuXrYIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building our network**\n",
        "\n",
        "In our previous example, we were using 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.\n",
        "\n",
        "For this reason we will use larger layers. Let's go with 64 units:"
      ],
      "metadata": {
        "id": "SGMXeIrurbab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "metadata": {
        "id": "Y7x1Loo0rZYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "activation function is relu : ReLU activation function is an activation function defined as the positive part of its argument: where x is the input to a neuron. For example, if x is a positive value then, f(x) = x otherwise f(x) = 0\n",
        "\n",
        "The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution"
      ],
      "metadata": {
        "id": "T9mkLIS_rg26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "XnW6-TnXrevC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rmsprop is similar to gradient descent algorithm with momentum.\n",
        "\n",
        "categorical cross entropy is used when the labels are one hot encoded."
      ],
      "metadata": {
        "id": "X9vV4xrarlCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validating our approach\n",
        "\n",
        "Let's set apart 1,000 samples in our training data to use as a validation set:"
      ],
      "metadata": {
        "id": "WVESR2uPrni6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "0ipQnLxZrjJj",
        "outputId": "2cb2f204-81f4-469f-e839-69d825b812cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-90a50ee7d258>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(partial_x_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(x_val, y_val))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'partial_x_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fmVa9VfsYSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                       plt.clf()   # clear figure\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4rrzi9JWrqRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on the test set:"
      ],
      "metadata": {
        "id": "g4O6NyJksKBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ],
      "metadata": {
        "id": "je-gUMlxsMLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "bQsj92QjsNlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}